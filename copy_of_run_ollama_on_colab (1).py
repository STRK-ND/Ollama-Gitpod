# -*- coding: utf-8 -*-
"""Copy of run-ollama-on-colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ihiaExZTRTib0Z4kuofX6WYEAJ-YwNEo
"""

!pip install pyngrok

import asyncio
import os
from google.colab import userdata
import aiohttp
from pyngrok import ngrok

# Get Ngrok authentication token from colab secrets environment
NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')

# Set LD_LIBRARY_PATH for NVIDIA library
os.environ['LD_LIBRARY_PATH'] = '/usr/lib64-nvidia'

async def run(cmd):
    print('>>> starting', *cmd)
    p = await asyncio.subprocess.create_subprocess_exec(
        *cmd,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE
    )

    async def pipe(lines):
        async for line in lines:
            print(line.strip().decode('utf-8'))

    await asyncio.gather(
        pipe(p.stdout),
        pipe(p.stderr),
    )

async def setup_ollama():
    # Download and run the Ollama Linux install script
    await run(['sh', '-c', 'curl -fsSL https://ollama.com/install.sh | sh'])

async def setup_ngrok():
    # Authenticate with Ngrok
    await run(['ngrok', 'config', 'add-authtoken', NGROK_AUTH_TOKEN])

async def start_servers():
    await asyncio.gather(
        run(['ollama', 'serve']),
        run(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header', 'localhost:11434', '--domain', 'solid-hugely-gorilla.ngrok-free.app']),
    )

async def main():
    # Install required packages
    await run(['pip', 'install', 'aiohttp', 'pyngrok'])

    # Setup Ollama and Ngrok
    await setup_ollama()
    await setup_ngrok()

    # Start Ollama and Ngrok servers
    await start_servers()

# Use this to run the async code
await main()